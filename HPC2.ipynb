{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3JCfQJD51dE",
        "outputId": "786ba536-d10f-4e60-f666-3d23c619a962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/afnan47/cuda.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RtYZZxq58pr",
        "outputId": "5e7a4d40-99c5-4fa4-df32-0a25be625b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/afnan47/cuda.git\n",
            "  Cloning https://github.com/afnan47/cuda.git to /tmp/pip-req-build-5xhag49t\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/afnan47/cuda.git /tmp/pip-req-build-5xhag49t\n",
            "  Resolved https://github.com/afnan47/cuda.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4289 sha256=03ebb4b4d5e3f09c4c980fea1be6adb3026656c67efa90ef7163d245e3889a15\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-26ove_bu/wheels/aa/f3/44/e10c1d226ec561d971fcd4b0463f6bff08602afa928a3e7bc7\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3GEh_lC58ZU",
        "outputId": "6ae96f19-7bbb-4707-f27a-55d5ceb99816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPufGHZP5nBd",
        "outputId": "cac20eeb-b35e-4164-f46f-10e833d4006c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bubble.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile bubble.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "using namespace std;\n",
        "__device__ void device_swap(int& a, int& b) {\n",
        "int temp = a;\n",
        "a = b;\n",
        "b = temp;\n",
        "}\n",
        "__global__ void kernel_bubble_sort_odd_even(int* arr, int size) {\n",
        "bool isSorted = false;\n",
        "while (!isSorted) {\n",
        "isSorted = true;\n",
        "int tid = blockIdx.x * blockDim.x + threadIdx.x; //calculating gloabl thread id.\n",
        "if (tid % 2 == 0 && tid < size - 1) {\n",
        "if (arr[tid] > arr[tid + 1]) {\n",
        "device_swap(arr[tid], arr[tid + 1]);\n",
        "isSorted = false;\n",
        "}\n",
        "}\n",
        "__syncthreads(); // Synchronize threads within block\n",
        "if (tid % 2 != 0 && tid < size - 1) {\n",
        "if (arr[tid] > arr[tid + 1]) {\n",
        "device_swap(arr[tid], arr[tid + 1]);\n",
        "isSorted = false;\n",
        "}\n",
        "}\n",
        "__syncthreads(); // Synchronize threads within block\n",
        "}\n",
        "}\n",
        "void bubble_sort_odd_even(vector<int>& arr) {\n",
        "int size = arr.size();\n",
        "int* d_arr;\n",
        "cudaMalloc(&d_arr, size * sizeof(int));\n",
        "cudaMemcpy(d_arr, arr.data(), size * sizeof(int),cudaMemcpyHostToDevice);\n",
        "// Calculate grid and block dimensions\n",
        "int blockSize = 256;\n",
        "int gridSize = (size + blockSize - 1) / blockSize;\n",
        "// Perform bubble sort on GPU\n",
        "kernel_bubble_sort_odd_even<<<gridSize, blockSize>>>(d_arr, size);\n",
        "// Copy sorted array back to host\n",
        "cudaMemcpy(arr.data(), d_arr, size * sizeof(int),cudaMemcpyDeviceToHost);\n",
        "cout<<\"sorted array\"<<endl;\n",
        "for(int i=0;i<size;i++){\n",
        "cout<<arr[i]<<\" \";\n",
        "}\n",
        "cout<<endl;\n",
        "cudaFree(d_arr);\n",
        "}\n",
        "int main() {\n",
        "vector<int> arr = {5,4,3,2,1,0,6,9,7 };\n",
        "double start, end;\n",
        "// Measure performance of parallel bubble sort using odd-even transposition\n",
        "start = chrono::duration_cast<chrono::milliseconds>(chrono::system_clock::now().time_since_epoch()).count();\n",
        "bubble_sort_odd_even(arr);\n",
        "end = chrono::duration_cast<chrono::milliseconds>(chrono::system_clock::now().time_since_epoch()).count();\n",
        "cout << \"Parallel bubble sort using odd-even transposition time: \" <<end - start << \" milliseconds\" << endl;\n",
        "return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc bubble.cu -o bubble\n",
        "!./bubble"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5Wb3xtp5tko",
        "outputId": "f9678563-b8a2-4dee-cbbd-cd70f5de4b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sorted array\n",
            "0 1 2 3 4 5 6 7 9 \n",
            "Parallel bubble sort using odd-even transposition time: 476 milliseconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile merge_sort.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "#include <algorithm> // for min function\n",
        "using namespace std;\n",
        "// Kernel to merge two sorted halves\n",
        "__global__ void kernel_merge(int* arr, int* temp, int* subarray_sizes, int array_size) {\n",
        "int idx = blockIdx.x * blockDim.x + threadIdx.x;//calculating global thread id\n",
        "int left_start = idx * 2 * (*subarray_sizes);\n",
        "if (left_start < array_size) {\n",
        "int mid = min(left_start + (*subarray_sizes) - 1, array_size - 1);\n",
        "int right_end = min(left_start + 2 * (*subarray_sizes) - 1,\n",
        "array_size - 1);\n",
        "int i = left_start;\n",
        "int j = mid + 1;\n",
        "int k = left_start;\n",
        "// Merge process\n",
        "while (i <= mid && j <= right_end) {\n",
        "if (arr[i] <= arr[j]) {\n",
        "temp[k] = arr[i];\n",
        "i++;\n",
        "} else {\n",
        "temp[k] = arr[j];\n",
        "j++;\n",
        "}\n",
        "k++;\n",
        "}\n",
        "while (i <= mid) {\n",
        "temp[k] = arr[i];\n",
        "i++;\n",
        "k++;\n",
        "}\n",
        "while (j <= right_end) {\n",
        "temp[k] = arr[j];\n",
        "j++;\n",
        "k++;\n",
        "}\n",
        "// Copy the sorted subarray back to the original array\n",
        "for (int t = left_start; t <= right_end; t++) {\n",
        "arr[t] = temp[t];\n",
        "}\n",
        "}\n",
        "}\n",
        "void merge_sort(vector<int>& arr) {\n",
        "int array_size = arr.size();\n",
        "int* d_arr;\n",
        "int* d_temp;\n",
        "int* d_subarray_size;\n",
        "// Allocate memory on the GPU\n",
        "cudaMalloc(&d_arr, array_size * sizeof(int));\n",
        "cudaMalloc(&d_temp, array_size * sizeof(int));\n",
        "cudaMalloc(&d_subarray_size, sizeof(int)); // Holds the subarray size for each step\n",
        "cudaMemcpy(d_arr, arr.data(), array_size * sizeof(int),cudaMemcpyHostToDevice);\n",
        "int blockSize = 256; // Threads per block\n",
        "int gridSize; // Number of blocks in the grid, depending on the subarray size\n",
        "// Start with width of 1, then double each iteration\n",
        "int width = 1;\n",
        "while (width < array_size) {\n",
        "cudaMemcpy(d_subarray_size, &width, sizeof(int),cudaMemcpyHostToDevice);\n",
        "gridSize = (array_size / (2 * width)) + 1;\n",
        "kernel_merge<<<gridSize, blockSize>>>(d_arr, d_temp,\n",
        "d_subarray_size, array_size);\n",
        "cudaDeviceSynchronize(); // Ensure all threads finish before the next step\n",
        "// Double the subarray width for the next iteration\n",
        "width *= 2;\n",
        "}\n",
        "// Copy the sorted array back to the host\n",
        "cudaMemcpy(arr.data(), d_arr, array_size * sizeof(int),\n",
        "cudaMemcpyDeviceToHost);\n",
        "// Free GPU memory\n",
        "cudaFree(d_arr);\n",
        "cudaFree(d_temp);\n",
        "cudaFree(d_subarray_size);\n",
        "}\n",
        "int main() {\n",
        "vector<int> arr = {6, 5, 4, 1, 7, 9, 8, 3, 2};\n",
        "double start, end;\n",
        "start =chrono::duration_cast<chrono::milliseconds>(chrono::system_clock::now().time_since_epoch()).count();\n",
        "merge_sort(arr);\n",
        "end =chrono::duration_cast<chrono::milliseconds>(chrono::system_clock::now().time_since_epoch()).count();\n",
        "cout << \"Parallel merge sort time: \" << end - start << \" milliseconds\"<< endl;\n",
        "cout << \"Sorted array: \";\n",
        "for (int num : arr) {\n",
        "cout << num << \" \";\n",
        "}\n",
        "cout << endl;\n",
        "return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgZQxeLN5yt2",
        "outputId": "f80b4d86-dc8a-4bba-9b73-6ac24d02bef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting merge_sort.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc merge_sort.cu -o merge\n",
        "!./merge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlL3AewD6iQR",
        "outputId": "b78e567d-9327-4268-ea74-144852ab122f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parallel merge sort time: 255 milliseconds\n",
            "Sorted array: 1 2 3 4 5 6 7 8 9 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EEopFmFu6k4C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}